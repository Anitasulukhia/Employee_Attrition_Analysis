{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c585485",
   "metadata": {},
   "source": [
    "now, as we have already analyzed data and made some meaningful changes, we move onto and this file implements: preparing data specifically for machine learning : feature selection, encoding, class imbalance treating, implementing multiple ML models and evaluating them\n",
    "\n",
    "\n",
    "✅ Baseline model\n",
    "No resampling\n",
    "Proper metrics\n",
    "Show imbalance problem\n",
    "\n",
    "✅ Weighted model\n",
    "class_weight='balanced'\n",
    "✅ SMOTE model\n",
    "Compare recall / F1 / ROC-AUC\n",
    "✅ Explain trade-offs\n",
    "Precision vs recall\n",
    "Business interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc6104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have high class imbalance as already mentioned, so to show difference and importance of data balancing, we will train one logistic regression with unbalanced data,\n",
    "# and one with weighted model - comparing recall and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d0a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d34b05ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8809523809523809\n",
      "F1 score: 0.5070422535211268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       255\n",
      "           1       0.56      0.46      0.51        39\n",
      "\n",
      "    accuracy                           0.88       294\n",
      "   macro avg       0.74      0.70      0.72       294\n",
      "weighted avg       0.87      0.88      0.88       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first we will train model on given,  unbalanced data to really see the difference balancing makes.\n",
    "\n",
    "# Features and target\n",
    "X = data.drop(columns='Attrition')\n",
    "y = data['Attrition']  \n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression\n",
    "model = LogisticRegression(solver='lbfgs', random_state=0, max_iter=10000)  \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"F1 score:\", f1_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b121ef",
   "metadata": {},
   "source": [
    "we obviously see the terrible result and contrast in between accuracy and F1. accuracy is 0.88 while F1 score is 0.56, this is because our model mostly guess the majority class, so when minority class is prioritized, model performance drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cab58375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8469387755102041\n",
      "F1 score: 0.47058823529411764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       255\n",
      "           1       0.43      0.51      0.47        39\n",
      "\n",
      "    accuracy                           0.85       294\n",
      "   macro avg       0.68      0.71      0.69       294\n",
      "weighted avg       0.86      0.85      0.85       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Features and target\n",
    "\n",
    "X = data.drop(columns='Attrition')\n",
    "y = data['Attrition']  \n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression\n",
    "model2 = LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    random_state=0,\n",
    "    max_iter=5000,\n",
    "    class_weight={0:1, 1:2}\n",
    ")\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "predictions = model2.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"F1 score:\", f1_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e59df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8673469387755102\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93       255\n",
      "           1       0.50      0.08      0.13        39\n",
      "\n",
      "    accuracy                           0.87       294\n",
      "   macro avg       0.69      0.53      0.53       294\n",
      "weighted avg       0.83      0.87      0.82       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,      # number of trees\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    max_features=\"sqrt\",   # features per split\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
